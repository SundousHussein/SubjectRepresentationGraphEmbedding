{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importing Packages and Libraries\n",
    "import csv\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from progressbar import ProgressBar\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def evaluate_node_embeddings(subgraph, params):\n",
    "    \"\"\"\n",
    "    1. Runs Node2Vec Algorithm to Generate Nodes Embeddings.\n",
    "    2. Adds Nodes Embeddings to Original Dataset.\n",
    "    3. Evaluates the Quality of the Embeddings (Correlation of PC1 with the Phenotype & Percentage of Variance Explained by PC1).\n",
    "    :param params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    node2vec = Node2Vec(subgraph, dimensions=params['dimensions'], walk_length=params['walk_length'], num_walks=params['num_walks'], weight_key='weight', p=params['p'], q=params['q'], quiet=True)\n",
    "    # print(\"Running with the following Parameters:\\nDimensions: %s\\tWalk Length: %s\\tNum Walks: %s\\tp: %s\\tq: %s\" % (params['dimensions'], params['walk_length'], params['num_walks'], params['p'], params['q']))\n",
    "    try:\n",
    "        model = node2vec.fit()\n",
    "        model.wv.save_word2vec_format('SubgraphNodesEmbeddings8.embd')\n",
    "        subgraph_nodes_embeddings = pd.read_csv('SubgraphNodesEmbeddings8.embd', delim_whitespace=True, names = range(params['dimensions']), skiprows=1)\n",
    "        subgraph_nodes_embeddings = subgraph_nodes_embeddings.T\n",
    "\n",
    "        subgraph_nodes_dict_inv = dict((v, k) for k, v in subgraph_nodes_dict.items())\n",
    "        original_dataset_with_embeddings_df = pd.DataFrame(index=range(0, len(original_dataset)))\n",
    "        columns_names = original_dataset.columns\n",
    "\n",
    "        for column_name in columns_names:\n",
    "            # Get the Metabolite or Protein Embedding Created using Node2Vec\n",
    "            subgraph_node_embedding = subgraph_nodes_embeddings[subgraph_nodes_dict_inv[column_name]]\n",
    "            column_value = original_dataset[column_name]\n",
    "            z = [[vy * vx for ix, vx in enumerate(subgraph_node_embedding)] for iy, vy in enumerate(column_value)]\n",
    "            original_dataset_with_embeddings_df = pd.concat([original_dataset_with_embeddings_df, pd.DataFrame(z)], axis=1)\n",
    "\n",
    "        original_dataset_with_embeddings_df.columns = range(0, len(original_dataset.columns)*params['dimensions'])\n",
    "\n",
    "        pca = PCA()\n",
    "        pipe = Pipeline([('scaler', StandardScaler()), ('pca', pca)])\n",
    "        Xt = pca.fit_transform(original_dataset_with_embeddings_df)\n",
    "        PC1 = Xt[:,0]\n",
    "\n",
    "        explained_variance_ratio = pca.explained_variance_ratio_\n",
    "        print(\"Explained Variance Ratio by PC1 %s\" % explained_variance_ratio[0])\n",
    "        print(\"Explained Variance Ratio by the First 10 PCs %s\" % sum(explained_variance_ratio[:10]))\n",
    "        PC1_df = pd.DataFrame(PC1)\n",
    "        PC1_correlation_with_phenotype = PC1_df.corrwith(dataset_associated_phenotype['pctEmph']).tolist()[0]\n",
    "        PC1_correlation_with_phenotype = PC1_df.corrwith(dataset_associated_phenotype['FEV1pp_utah']).tolist()[0]\n",
    "        print(\"Correlation with the Phenotype: %s\" % PC1_correlation_with_phenotype)\n",
    "        return [params, PC1_correlation_with_phenotype, explained_variance_ratio[0], sum(explained_variance_ratio[:10])]\n",
    "    except Exception as E:\n",
    "        print(\"Exception %s\" % E)\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Node2Vec Hyperparameters Tuning\n",
    "\n",
    "param_grid = {\n",
    "    'dimensions': [2**p for p in range(1, 10)],\n",
    "    'walk_length' : [2**p for p in range(1, 10)],\n",
    "    'num_walks':  [2**p for p in range(1, 10)],\n",
    "    'p': [0.0001, 0.005, 0.01, 0.4, 2, 10, 100],\n",
    "    'q': [0.0001, 0.005, 0.01, 0.4, 2, 10, 100],\n",
    "    'window_size': list(range(1, 101)),\n",
    "    'iter': list(range(1, 1000))\n",
    "}\n",
    "grid = ParameterGrid(param_grid)\n",
    "# Subgraph Adjacency Matrix\n",
    "subgraph_adj = pd.read_csv('../Data/trimmed_FEV1_0.55_Adjacency.csv', index_col=0).to_numpy()\n",
    "subgraph = nx.from_numpy_matrix(subgraph_adj)\n",
    "\n",
    "# Reading the Input Dataset\n",
    "original_dataset = pd.read_csv('../Data/FEV1_X.csv', index_col=0).reset_index(drop='index')\n",
    "# Reading the Associated Phenotype\n",
    "dataset_associated_phenotype = pd.read_csv('../Data/FEV1_Y.csv', index_col=0).reset_index(drop='index')\n",
    "\n",
    "# Extracting the Network Nodes Names\n",
    "subgraph_nodes_dict = {}\n",
    "subgraph_nodes_names = []\n",
    "for subgraph_node in subgraph.nodes():\n",
    "    subgraph_node_name = original_dataset.iloc[:0, subgraph_node].name\n",
    "    subgraph_nodes_names.append(subgraph_node_name)\n",
    "    subgraph_nodes_dict[subgraph_node] = subgraph_node_name\n",
    "\n",
    "evaluation_results = []\n",
    "pbar = ProgressBar()\n",
    "for params in pbar(grid):\n",
    "    evaluation_results.append(evaluate_node_embeddings(subgraph, params))\n",
    "\n",
    "evaluation_results = [x for x in evaluation_results if x is not None]\n",
    "with open('TmpFile1.csv', 'w+', newline='') as evaluation_results_file:\n",
    "    writer = csv.writer(evaluation_results_file)\n",
    "    writer.writerows(evaluation_results)\n",
    "\n",
    "with open('TmpFile1.csv', 'r') as evaluation_results_file:\n",
    "    evaluation_results = list(csv.reader(evaluation_results_file))\n",
    "\n",
    "evaluation_results_list = []\n",
    "for evaluation_result in evaluation_results:\n",
    "    evaluation_result_list = []\n",
    "    evaluation_result_list += list(eval(evaluation_result[0]).values())\n",
    "    for item in evaluation_result[1:4]:\n",
    "        evaluation_result_list.append(item)\n",
    "    evaluation_results_list.append(evaluation_result_list)\n",
    "evaluation_results_df = pd.DataFrame(evaluation_results_list, columns=['dimensions', 'num_walk', 'p', 'q', 'walk_length', 'PC1_correlation_with_phenotype', 'explained_variance_ratio_PC1', 'explained_variance_ratio_10_PCs'])\n",
    "\n",
    "evaluation_results_df = evaluation_results_df.astype(float).abs()\n",
    "evaluation_results_df.to_csv('TwelvethExp.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Running Node2Vec to Obtain Node Embedding\n",
    "node2vec = Node2Vec(subgraph, dimensions=params['dimensions'], walk_length=params['walk_length'], num_walks=params['num_walks'], weight_key='weight', p=params['p'], q=params['q'], quiet=True)\n",
    "# print(\"Running with the following Parameters:\\nDimensions: %s\\tWalk Length: %s\\tNum Walks: %s\\tp: %s\\tq: %s\" % (params['dimensions'], params['walk_length'], params['num_walks'], params['p'], params['q']))\n",
    "try:\n",
    "    model = node2vec.fit()\n",
    "    model.wv.save_word2vec_format('SubgraphNodesEmbeddings8.embd')\n",
    "    subgraph_nodes_embeddings = pd.read_csv('SubgraphNodesEmbeddings8.embd', delim_whitespace=True, names = range(params['dimensions']), skiprows=1)\n",
    "except Exception as E:\n",
    "    print(\"Exception %s\" % E)\n",
    "subgraph_nodes_embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
